<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-09-28T12:21:38+01:00</updated><id>http://localhost:4000/</id><title type="html">tamas.dev</title><subtitle></subtitle><entry><title type="html">Run your ECS cluster with service discovery and HAProxy for $8 a month</title><link href="http://localhost:4000/aws/ecs/haproxy/nginx/certbot/microservices/containers/route53/dns/srv/2019/10/18/run-your-ecs-cluster-with-service-discovery-and-haproxy-for-8-a-month.html" rel="alternate" type="text/html" title="Run your ECS cluster with service discovery and HAProxy for $8 a month" /><published>2019-10-18T07:30:00+01:00</published><updated>2019-10-18T07:30:00+01:00</updated><id>http://localhost:4000/aws/ecs/haproxy/nginx/certbot/microservices/containers/route53/dns/srv/2019/10/18/run-your-ecs-cluster-with-service-discovery-and-haproxy-for-8-a-month</id><content type="html" xml:base="http://localhost:4000/aws/ecs/haproxy/nginx/certbot/microservices/containers/route53/dns/srv/2019/10/18/run-your-ecs-cluster-with-service-discovery-and-haproxy-for-8-a-month.html">&lt;h2 id=&quot;keeping-things-simple&quot;&gt;Keeping things simple&lt;/h2&gt;
&lt;p&gt;When working on side projects I always try to keep things simple and focus on the important things at the beginning as delivering values is key. I don’t want to deal with clusters, HA, pipelines etc. because it’s more than alright to deploy to production from your laptop in the early days. In this particular example, the infrastructure is basically an RDS instance (no multi AZ at this point) and an EC2 box with Ubuntu and docker installed on it manually. The deploy process is a simple shell script on my laptop that builds the container, uploads it to the registry, dials in to the remote box via SSH and does a docker pull. Super simple. However as time passes and the project gets traction, it’s getting more and more pressing to come up with at least a semi-scalable solution which is what this post is about.&lt;/p&gt;

&lt;p&gt;I recently got to a point where I wanted to introduce multiple containers (Django web server, celery worker, celery beat). Sure, I could just add another build and pull command to my existing script, but I figured maybe there’s a simple way to do this in a more proper fashion without blowing the budget. ECS seemed like a good choice (EC2 based on Fargate seems to be more expensive at this small scale) because it deals with deployment, container scheduling, logging, monitoring, etc. My only worry was that putting a load balancer in front of the services would increase the monthly budget by 50% (again, we’re talking very small scale), therefore I started looking for alternate solutions. And that’s when I found a solution provided by ECS itself.&lt;/p&gt;

&lt;h2 id=&quot;service-discovery-à-la-ecs&quot;&gt;Service discovery à la ECS&lt;/h2&gt;
&lt;p&gt;As it turns out, ECS supports service discovery out of the box, which is great as it solves half of my problem. If service discovery is enabled, a private Route53 zone is created (and managed by the service discovery service) and whenever a new container is scheduled, a new SRV and A record will be created automagically. I didn’t know this before, but using an SRV DNS record, you can define not just the target location, but also the target port. This solution works beautifully with dynamic port allocation which allows running multiple instances of the same service on a single physical box. The other huge benefit of this implementation is that there’s no Elastic Load Balancer involved, it all relies on DNS records and them being up to date. Think of it as a client side load balancing. And this is where I need a solution for the other half of the problem. I will need a reverse proxy in front of the services in order to resolve the DNS entries and to provide a single port exposed to the outside world. And this is where HAProxy comes to play.&lt;/p&gt;

&lt;p&gt;I’ll be honest here, I don’t have any prior experience with HAProxy so please don’t take any of the remaining post as a guide or as a best practice. The goal here is to explore possibilities that can save you a couple of bucks (literally a couple) at the early phase of your project. My first choice was Nginx as I have some experience with it. Unfortunately it does not support this use case only in their Nginx Plus offering, therefore HAProxy it is.&lt;/p&gt;

&lt;p&gt;As you’ll see later it’s fairly straight forward to set it up and configure, also it’s still just a proxy server at the end of the day with load balancing, so if you’re familiar with the concept, there’s nothing new here really.&lt;/p&gt;

&lt;p&gt;One more important thing is that if you get to a point where you need a more reliable solution, it’s very easy to just reconfigure the ECS service to use and actual load balancer.&lt;/p&gt;

&lt;h2 id=&quot;scope-of-this-post&quot;&gt;Scope of this post&lt;/h2&gt;
&lt;p&gt;In this post I’d like to walk you through a simple concept, which I’m sure is not perfect, so please do let me know if I missed something!&lt;/p&gt;

&lt;p&gt;The goal will be to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Create an EC2 spot instance based ECS cluster&lt;/li&gt;
  &lt;li&gt;Create a simple http hello-world ECS service with service discovery enabled&lt;/li&gt;
  &lt;li&gt;Create an EC2 box that’ll host HAProxy&lt;/li&gt;
  &lt;li&gt;Configure HAProxy and set up SSL termination using Let’s Encrypt’s certbot&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setting-up-ecs&quot;&gt;Setting up ECS&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Without going into too much detail on how to use ECS, let’s create a new ECS cluster, select EC2 Linux + Networking and use the following configuration:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Provisioning model: Spot&lt;/li&gt;
  &lt;li&gt;Allocation strategy: Lowest price&lt;/li&gt;
  &lt;li&gt;EC2 instance types: t2.micro, t2.small, t3.micro, t3.small&lt;/li&gt;
  &lt;li&gt;Number of instances should just be 1 at this point&lt;/li&gt;
  &lt;li&gt;Create a new VPC&lt;/li&gt;
  &lt;li&gt;Either select an existing or create new container instance and spot fleet IAM roles&lt;/li&gt;
  &lt;li&gt;No need to enable Container Insights&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Once the cluster is ready, create a task definition for our hello-world service. Again, without going into too much detail, these are the key configuration points:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Name: hello-world&lt;/li&gt;
  &lt;li&gt;Compatibilities / Requires compatibilities: EC2&lt;/li&gt;
  &lt;li&gt;Container definitions:
    &lt;ul&gt;
      &lt;li&gt;image: strm/helloworld-http&lt;/li&gt;
      &lt;li&gt;port mappings:
        &lt;ul&gt;
          &lt;li&gt;containerPort: 80&lt;/li&gt;
          &lt;li&gt;hostPort: 0 &lt;strong&gt;&amp;lt;= This will specify dynamic port allocation on the host&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; With a cluster and a task definition we can now create the service:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Launch type: EC2&lt;/li&gt;
  &lt;li&gt;Task definition / revision: hello-world (latest)&lt;/li&gt;
  &lt;li&gt;Cluster: select your cluster created in step 1&lt;/li&gt;
  &lt;li&gt;Service name: hello-world-service&lt;/li&gt;
  &lt;li&gt;Service type: Replica&lt;/li&gt;
  &lt;li&gt;Number of tasks: 3 &lt;strong&gt;&amp;lt;= In order to test load balancing&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Deployment type: Rolling update&lt;/li&gt;
  &lt;li&gt;Load balancer type: None &lt;strong&gt;&amp;lt;= Whole point of this post&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Enable service discovery integration: checked&lt;/li&gt;
  &lt;li&gt;Namespace: create new private namespace&lt;/li&gt;
  &lt;li&gt;Namespace name: internal&lt;/li&gt;
  &lt;li&gt;Cluster VPC: select the VPC that belongs to the cluster created in step 1&lt;/li&gt;
  &lt;li&gt;Configure service discovery service: Create new service discovery service&lt;/li&gt;
  &lt;li&gt;Service discovery name: _hello-world-service &lt;strong&gt;&amp;lt;= Very important: HAProxy will only resolve the SRV DNS correctly if it starts with an underscore&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Enable ECS task health propagation: checked&lt;/li&gt;
  &lt;li&gt;Service Auto Scaling: Don not adjust the service’s desired count&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://tamas.dev/static/2019-10-17/service_discovery_records.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://tamas.dev/static/2019-10-17/service_discovery_records.png&quot; alt=&quot;SRV and A records created by service discovery service&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;setting-up-haproxy&quot;&gt;Setting up HAProxy&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Create an EC2 box. Here I just simply created a t3a.nano EC2 instance with Ubuntu 18.04 LTS. The important bit here is to make sure to place the instance in the same VPC as the ECS instances.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://tamas.dev/static/2019-10-17/ec2_config.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://tamas.dev/static/2019-10-17/ec2_config.png&quot; alt=&quot;EC2 configuration&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Configure security groups. In order to be able to access all ports on the instances, you’ll need to add an Inbound rule to the security group used by the ECS instances. The rule should allow All TCP traffic coming from the security group created for the HAProxy instance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://tamas.dev/static/2019-10-17/sg_config.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://tamas.dev/static/2019-10-17/sg_config.png&quot; alt=&quot;Security group configuration&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; At this point we can test if we have access to the services and if the DNS resolution is working correctly:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SSH into the EC2 box we’ve just created&lt;/li&gt;
  &lt;li&gt;Test if &lt;code class=&quot;highlighter-rouge&quot;&gt;dig srv _hello-world-service.internal&lt;/code&gt; returns the 3 SRV records we expect&lt;/li&gt;
  &lt;li&gt;Test if the record is accesible on the specified port, ie.: &lt;code class=&quot;highlighter-rouge&quot;&gt;curl 703a...c6d._hello-world-service.internal:32770&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://tamas.dev/static/2019-10-17/test_dig_curl.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://tamas.dev/static/2019-10-17/test_dig_curl.png&quot; alt=&quot;Test DNS and access with dig and curl&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you get a HTML response with Hello World, all is good so far.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt; Time do set up Let’s Encrypt. My preferred way of doing that on AWS is via DNS so there’s no need to shut down the proxy server, or to have certbot specific route configurations. Basically certbot will use the machined IAM role to write some entries into the domain’s Hosted Zone in Route 53. The IAM role that you need to create and attach to the EC2 box is the following:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Id&quot;: &quot;certbot-dns-route53 policy&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [
                &quot;route53:ListHostedZones&quot;,
                &quot;route53:GetChange&quot;
            ],
            &quot;Resource&quot;: [
                &quot;*&quot;
            ]
        },
        {
            &quot;Effect&quot; : &quot;Allow&quot;,
            &quot;Action&quot; : [
                &quot;route53:ChangeResourceRecordSets&quot;
            ],
            &quot;Resource&quot; : [
                &quot;arn:aws:route53:::hostedzone/YOURHOSTEDZONEID&quot;
            ]
        }
    ]
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Don’t forget to add your Route53 Hosted Zone ID to the resources section.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5.&lt;/strong&gt; Time to install HAProxy and certbot:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo add-apt-repository -y ppa:vbernat/haproxy-2.0
sudo add-apt-repository -y ppa:certbot/certbot
sudo apt-get update
sudo apt-get install -y software-properties-common certbot python3-certbot-dns-route53 haproxy=2.0.\* 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;6.&lt;/strong&gt; Let’s run certbot and prepare certificates for HAProxy:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;certbot certonly -d *.yourdomain.com -d yourdomain.com \
    --dns-route53 \
    -m your.email@email.com \
    --agree-tos \
    --non-interactive \
    --server https://acme-v02.api.letsencrypt.org/directory \
    --logs-dir ~/letsencrypt/log/ \
    --config-dir ~/letsencrypt/config/ \
    --work-dir ~/letsencrypt/work/

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the certificates are generated, you need to concatenate them for HAProxy:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo mkdir -p /etc/haproxy/certs/
DOMAIN='yourdomain.com' sudo -E bash -c 'cat ./letsencrypt/config/live/$DOMAIN/fullchain.pem ./letsencrypt/config/live/$DOMAIN/privkey.pem &amp;gt; /etc/haproxy/certs/$DOMAIN.pem'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;7.&lt;/strong&gt; Configure HAProxy: Open HAProxy configuration with your favourite editor (&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo nano /etc/haproxy/haproxy.cfg&lt;/code&gt;) and append the following at the end of the file:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# In case it's a simple http call, we redirect to the basic backend server
# which in turn, if it isn't an SSL call, will redirect to HTTPS that is
# handled by the frontend setting called 'www-https'.
frontend www-http
    # Redirect HTTP to HTTPS
    bind *:80
    # Adds http header to end of end of the HTTP request
    http-request add-header X-Forwarded-Proto http
    # Sets the default backend to use which is defined below with name 'www-backend'
    default_backend www-backend

# If the call is HTTPS we set the certificate and direct traffic to the 
# backend server.
frontend www-https
    # Bind 443 with the generated letsencrypt cert.
    bind *:443 ssl crt /etc/haproxy/certs/yourdomain.com.pem
    # set x-forward to https
    http-request add-header X-Forwarded-Proto https
    # set X-SSL in case of ssl_fc &amp;lt;- explained below
    http-request set-header X-SSL %[ssl_fc]
    default_backend www-backend

resolvers awsvpc
  # Your nameserver address should always be your VPC CIDR block +2
  # (in this case 10.0.0.0 + 2 = 10.0.0.2) and port 53
  nameserver dns1 10.0.0.2:53
  resolve_retries 3
  timeout retry 1s
  # allow larger DNS payloads due to multiple entries
  accepted_payload_size 8192

backend www-backend
   # Redirect with code 301 so the browser understands it is a redirect. If it's not SSL_FC.
   # ssl_fc: Returns true when the front connection was made via an SSL/TLS transport
   # layer and is locally deciphered. This means it has matched a socket declared
   # with a &quot;bind&quot; line having the &quot;ssl&quot; option.
   redirect scheme https code 301 if !{ ssl_fc }
   # Servers for the running ECS service:
   server-template srv 3 _hello-world-service.internal check resolvers awsvpc resolve-opts allow-dup-ip init-addr last,libc,none

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A little explanation here: The key configuration bit for this use case is HAProxy’s &lt;code class=&quot;highlighter-rouge&quot;&gt;server-template&lt;/code&gt; directive that adds the backend servers dynamically, base on the response coming from the DNS server.&lt;/p&gt;

&lt;p&gt;Once done, just restart HAProxy &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo service haproxy restart&lt;/code&gt; and everything should be good to go!&lt;/p&gt;

&lt;h2 id=&quot;like-magic-but-better&quot;&gt;Like magic, but better!&lt;/h2&gt;

&lt;p&gt;Once the server has been restarted, you can open your browser and visit your domain. You should see a &lt;em&gt;Hello from XYZ&lt;/em&gt; message where XYZ is a hash of the server’s container. Since the load is spread across multiple instances, every time you refresh the page, the target container can be a different one from the previous page load. I really hope you’ve found this post useful and if you’d like to do any further research, please see the resources I used below. Also, just to get a sense of scale and pricing, find below the current, the updated and the ELB based monthly charges.&lt;/p&gt;

&lt;h2 id=&quot;monthly-charges&quot;&gt;Monthly charges&lt;/h2&gt;

&lt;h4 id=&quot;current-monthly-charges&quot;&gt;Current monthly charges:&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1x t2.micro on demand:
$0.0132USD * 24 hours * 30 days = $9.5 per month
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Total: &lt;strong&gt;$9.504&lt;/strong&gt; per month&lt;/p&gt;

&lt;h4 id=&quot;updated-monthly-charges&quot;&gt;Updated monthly charges:&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1x t3a.nano on demand (proxy server):
$0.0053 * 24 * 30 = $3.816 per month

1x t3a.micro spot (ECS instance):
~$0.0031 * 24 * 30 = $2.232 per month

Route53 and Cloud Map charges:
~$1.5 per month
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Total: &lt;strong&gt;$7.548&lt;/strong&gt; per month (yes, it means paying less for a more managed, more scalable solution)&lt;/p&gt;

&lt;h4 id=&quot;monthly-charges-using-an-elb&quot;&gt;Monthly charges using an ELB:&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1x t3a.micro spot (ECS instance):
~$0.0031 * 24 * 30 = $2.232 per month

1x Application Load Balancer:
$0.02646 * 24 * 30 = $19.0512 per month + LCU-hour charges
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Total: &lt;strong&gt;$21.2832&lt;/strong&gt; per month (+ LCU-hour charges)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Resources used:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.haproxy.com/blog/dns-service-discovery-haproxy/&quot;&gt;DNS for Service Discovery in HAProxy&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://skarlso.github.io/2017/02/15/how-to-https-with-hugo-letsencrypt-haproxy/&quot;&gt;How to HTTPS with Hugo LetsEncrypt and HAProxy&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Keeping things simple When working on side projects I always try to keep things simple and focus on the important things at the beginning as delivering values is key. I don’t want to deal with clusters, HA, pipelines etc. because it’s more than alright to deploy to production from your laptop in the early days. In this particular example, the infrastructure is basically an RDS instance (no multi AZ at this point) and an EC2 box with Ubuntu and docker installed on it manually. The deploy process is a simple shell script on my laptop that builds the container, uploads it to the registry, dials in to the remote box via SSH and does a docker pull. Super simple. However as time passes and the project gets traction, it’s getting more and more pressing to come up with at least a semi-scalable solution which is what this post is about.</summary></entry><entry><title type="html">Adventures with Cloudfront 502s</title><link href="http://localhost:4000/aws/cloudfront/elb/alb/ssl/certificate/2019/07/17/adventures-with-cloudfront-502s.html" rel="alternate" type="text/html" title="Adventures with Cloudfront 502s" /><published>2019-07-17T09:30:00+01:00</published><updated>2019-07-17T09:30:00+01:00</updated><id>http://localhost:4000/aws/cloudfront/elb/alb/ssl/certificate/2019/07/17/adventures-with-cloudfront-502s</id><content type="html" xml:base="http://localhost:4000/aws/cloudfront/elb/alb/ssl/certificate/2019/07/17/adventures-with-cloudfront-502s.html">&lt;h2 id=&quot;the-plan&quot;&gt;The plan&lt;/h2&gt;
&lt;p&gt;Dealing with legacy code is never boring, especially if you want to change something fundamental in the app. We decided that it was time to detach the frontend application from the backend service. In the old setup we had a Django application responsible to serve the API endpoints + it served a static HTML page hosting the frontend application. Even though the separation wasn’t super straight forward as some variables were passed to the frontend via Django template tags, refactoring them to be consumed by webpack from a settings.json file went fairly smoothly. The troubles began when it was time to separate the infrastructure. Without going into too much detail, the backend service was and would still be sitting behind an application load balancer while the frontend app would be uploaded to an S3 bucket both sitting behind a Cloudfront distribution. Nothing too crazy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://tamas.dev/static/infra_update.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://tamas.dev/static/infra_update.png&quot; alt=&quot;Updated infrastructure&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;domain-changes&quot;&gt;Domain changes&lt;/h2&gt;
&lt;p&gt;This also seemed to be a good time to update the website’s domain structure a little bit and get rid of the app.* subdomain. We have two QA, a staging environment and a production environment. The old domains were as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;qa1.mydomain.com&lt;/code&gt; (QA1)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;qa2.mydomain.com&lt;/code&gt; (QA2)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;app.staging.mydomain.com&lt;/code&gt; (staging)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;app.mydomain.com&lt;/code&gt; (production)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you can see there was some incosistency across the environments whether to use the app subdomain or not, so at least this work would resolve this as well. The plan was to update the routing to use either /app path for the static frontend app (S3 bucket) or /core where the APIs would be attached (via an ALB). This would result in the following routes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;qa1.mydomain.com/app&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;qa1.mydomain.com/core&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;qa2.mydomain.com/app&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;qa2.mydomain.com/core&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;staging.mydomain.com/app&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;staging.mydomain.com/core&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mydomain.com/app&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;mydomain.com/core&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These updates will become relevant later.&lt;/p&gt;

&lt;h2 id=&quot;qa-staging-production&quot;&gt;QA, staging, production&lt;/h2&gt;
&lt;p&gt;Once the necessary code updates have been implemented, it was time to deploy to QA. The buckets were created and configured, a Cloudfront distribution was created with the two origins (S3 and ALB), SSL certificate was issued. The application was then deployed and after a few initial issues, everything was working great! After having the solution deployed to QA for a while, we got pretty confident about the solution, and we decided it was time to merge the code changes and deploy to staging. Once deployed, no hiccups whatsoever, everything was working smoothly straight away. At this time the buckets and Cloufront distributions have already been created as a preparation for both staging and production environment.
It was then time to go live. We started the deployment process and once it was deployed, we switched our domains over to use the new cloudfront distribution instead of the load balancers…. Only to realise that all API endpoints are returning 502s.&lt;/p&gt;

&lt;h2 id=&quot;502&quot;&gt;502&lt;/h2&gt;
&lt;p&gt;Even though the frontend was loading perfectly and the login page was rendered without any issues, we immediately realised that users were not able to log in. Thankfully we timed this update outside core hours, so not many users were affected.&lt;/p&gt;

&lt;p&gt;502 is the error message that can mean a lot of things, so using our years of engineering experience, we managed to Google the issue fairly quickly. There’s a troubleshooting page dedicated to Cloudfront’s 502 response with list of things that could go wrong, so we started going through the list.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/premiumsupport/knowledge-center/resolve-cloudfront-connection-error/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://tamas.dev/static/cloudfront_502s.png&quot; alt=&quot;Cloudfront 502 troubleshooting guide&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It was pretty clear that the issue is not with Cloudfront itself, but either something with the backend service or something &lt;em&gt;between&lt;/em&gt; Cloudfront and our backend service. Some quick curl calls proved that the ALB is fine as it returned non-5xx responses. We then went through the origin and behaviour configuration for the backend service and all seemed to be correct too. We started to worry.&lt;/p&gt;

&lt;h2 id=&quot;domain-changes-part-2&quot;&gt;Domain changes part 2&lt;/h2&gt;
&lt;p&gt;Of course when we tried the curl calls, we just did a quick curl against the ALB DNS name provided by AWS. Which is by default http in curl, unless specified otherwise. In reality however, the Cloudfront origin was configured as https. After trying to curl via https (realising this is what the troubleshooting guide is trying to say), we finally managed to get an SSL error!&lt;/p&gt;

&lt;p target=&quot;_blank&quot;&gt;&lt;a href=&quot;https://www.reddit.com/r/me_irl/comments/86tb5k/meirl/&quot;&gt;&lt;img src=&quot;https://external-preview.redd.it/E896abucqJeMw3XvoqpZ4RMHbKBjRoqqDlpFwqECmtg.jpg?width=960&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52d8c00bf39e5759cec1be9938d515f2d81f4b63&quot; alt=&quot;Updated infrastructure&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p style=&quot;color:gray; font-size: 80%; text-align: center;&quot;&gt;&lt;em&gt;credit: https://www.reddit.com/r/me_irl/comments/86tb5k/meirl/&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Invalid certificates? How could that be? We haven’t updated anything certificate related other than issuing new Cloudfront certificates which were working just fine. All ALBs were configured to use the same certificate with a wildcard &lt;code class=&quot;highlighter-rouge&quot;&gt;*.mydomain.com&lt;/code&gt;. And there it was. &lt;code class=&quot;highlighter-rouge&quot;&gt;*.mydomain.com&lt;/code&gt; was a perfectly valid wildcard for all the old domains, also for the new QA and staging domains. However, since we’ve changed the production domain from &lt;em&gt;app.mydomain.com&lt;/em&gt; to &lt;em&gt;mydomain.com/core&lt;/em&gt;, the wildcard did not apply to that domain anymore as there were no more subdomains.&lt;/p&gt;

&lt;h2 id=&quot;why-was-only-the-backend-broken&quot;&gt;Why was only the backend broken?&lt;/h2&gt;
&lt;p&gt;If you’ve ever configure Cloudfront with SSL managed by AWS, you know that the certificates for all distributions have to be issued in the “us-east-1” region. When the new distributions were created, we created a new certificate for all domains. However, for the load balancer certificates you have to issue them in the VPC’s own region. Even though we did update some configuration on the ALBs (set up 301s for the old domain, update target group rules, etc.), we’ve completely forgotten about the certificates attached to the HTTPS listener. Especially since everything was working perfectly on QA and staging, we wouldn’t have thought there could be issues on production. Basically since the ALB’s certificate was invalid, the issue only affected the connections towards the backend.&lt;/p&gt;

&lt;h2 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h2&gt;
&lt;p&gt;For me the biggest takeaway from this incident is that I should always double-triple check SSL certificates when moving domains around. It’s easy to take things for granted, especially if things seem to work fine on QA and staging, but I should always consider the differences between these environments and production and their possible consequences.&lt;/p&gt;

&lt;p&gt;Also, error reporting with Cloudfront is not very great. I understand that this is kinda sensitive information and should not be revealed to visitors, however it would be great to somehow publish these error messages to Cloudwatch.&lt;/p&gt;</content><author><name></name></author><summary type="html">The plan Dealing with legacy code is never boring, especially if you want to change something fundamental in the app. We decided that it was time to detach the frontend application from the backend service. In the old setup we had a Django application responsible to serve the API endpoints + it served a static HTML page hosting the frontend application. Even though the separation wasn’t super straight forward as some variables were passed to the frontend via Django template tags, refactoring them to be consumed by webpack from a settings.json file went fairly smoothly. The troubles began when it was time to separate the infrastructure. Without going into too much detail, the backend service was and would still be sitting behind an application load balancer while the frontend app would be uploaded to an S3 bucket both sitting behind a Cloudfront distribution. Nothing too crazy.</summary></entry></feed>